{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Text Preprosessing Overlook\n",
    "\n",
    "This notebook is to try out the preprocessing on the input files.\n",
    "The after the tokeniser function finalised it is been put into the Preprocessor Module for use. The notebook then import Preprocessor.py to the notebook and test the functions on in the notebook\n",
    "\n",
    "Steps:\n",
    "Preprocessing:\n",
    "* tokenise http://www.nltk.org/api/nltk.tokenize.html\n",
    "* lower case\n",
    "* Remove puntuation, regonise links, AT users, \n",
    "* option to regonise/remove stop words\n",
    "* stemming and lemmatisation http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "* pick out emoticons\n",
    "* remove duplication characters (done as part of TwitterTokenizer)\n",
    "\n",
    "\n",
    "Feature extracting:\n",
    "* word exestence and count\n",
    "* tf, tfidf\n",
    "* count for hashtag, at, and link\n",
    "* n-gram: bigram, trigram and skipgram\n",
    "* lexicons and \n",
    "* word embedding\n",
    "\n",
    "http://www.nltk.org/api/nltk.sentiment.html  \n",
    "\n",
    "The tokenise function and extract_features function both have some arguments to control part of the preprocessing so we can change some parameters during text classification to find the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a stopwords corpus in nltk (installed by nltk.download()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print stopwords.words('english')[0:30]\n",
    "\n",
    "# nltk.download(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying and testing the tokenise function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'is', u'so', u'damn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords as stopwordloader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "t1 = \"@Harry_Styles is so damn cute!!! 1 day to go DIRECTIONERS!!!!!!!! It's coming out at 8\"\n",
    "t2 = \"I have one awesome boyfriend who Sat and watched jersey shore with me just to make me happy :)\"\n",
    "t3 = \"We're going to be in the rose parade tomorrow with the other obedience club dogs! Lizzie will be wearing roses! :-)\"\n",
    "t4 = \"New header of Lana Del Rey *__* her face is getting blocked though, I may change it later..\"\n",
    "t5 = \"she call me baby, I call her bae, sorry 4 the wait, Carter IV on the way! august 29th. hellllll yeahhh! (;\"\n",
    "t6 = \"RT @coupleBOOBS: Nicki Minaj - Pink Friday, up 15 places, to #16 on the Official UK Top 40 :) http://twitpic.com/62iohc -__-\"\n",
    "t7 = \"@kjrmitch you should call in sick....i mean who can listen to HM tomorrow.  Blah, blah...matt Flynn...blah, blah blah...matt Flynn....blah,\"\n",
    "t8 = \"Going to the UCLA football game this saturday with my boo . &lt;3  ^.^\\n\"\n",
    "t9 = \"sooo i might go to laidback luke on friday with nessaaaa ^.^ we going hard &gt;:]\\n\"\n",
    "\n",
    "# the tokenise function is written in Preprocessor.py, we import it here\n",
    "from Preprocessor import tokenise\n",
    "\n",
    "tokenise(t1, begin=1, end = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I', u'have', u'one', u'awesome', u'boyfriend', u'who', u'sat', u'and', u'watched', u'jersey', u'shore', u'with', u'me', u'just', u'to', u'make', u'me', u'happy', u':)']\n",
      "[u\"we're\", u'going', u'to', u'be', u'in', u'the', u'rose', u'parade', u'tomorrow', u'with', u'the', u'other', u'obedience', u'club', u'dogs', u'!', u'lizzie', u'will', u'be', u'wearing', u'roses', u'!', u':-)']\n",
      "[u'new', u'header', u'of', u'lana', u'del', u'rey', u'*__*', u'her', u'face', u'is', u'getting', u'blocked', u'though', u'I', u'may', u'change', u'it', u'later', u'..']\n",
      "[u'she', u'call', u'me', u'baby', u'I', u'call', u'her', u'bae', u'sorry', u'NUMBER', u'the', u'wait', u'carter', u'IV', u'on', u'the', u'way', u'!', u'august', u'NUMBER', u'helll', u'yeahhh', u'!', u'(;']\n",
      "[u'RT', u'AT_USER', u'nicki', u'minaj', u'pink', u'friday', u'up', u'NUMBER', u'places', u'to', u'#16', u'on', u'the', u'official', u'UK', u'top', u'NUMBER', u':)', u'LINK', u'-__-']\n",
      "[u'AT_USER', u'you', u'should', u'call', u'in', u'sick', u'...', u'i', u'mean', u'who', u'can', u'listen', u'to', u'HM', u'tomorrow', u'blah', u'blah', u'...', u'matt', u'flynn', u'...', u'blah', u'blah', u'blah', u'...', u'matt', u'flynn', u'...', u'blah']\n",
      "[u'going', u'to', u'the', u'UCLA', u'football', u'game', u'this', u'saturday', u'with', u'my', u'boo', u'<3', u'^.^']\n",
      "[u'sooo', u'i', u'might', u'go', u'to', u'laidback', u'luke', u'on', u'friday', u'with', u'nessaaa', u'^.^', u'we', u'going', u'hard', u'>:]']\n"
     ]
    }
   ],
   "source": [
    "# more testing \n",
    "print tokenise(t2)\n",
    "print tokenise(t3)\n",
    "print tokenise(t4)\n",
    "print tokenise(t5)\n",
    "print tokenise(t6)\n",
    "print tokenise(t7)\n",
    "print tokenise(t8)\n",
    "print tokenise(t9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add support for Japanese style Emoticons\n",
    "the nltk TweetTokenizer which our tokeniser is based on work well in picking up western style emoticons, however it does not cover Japanese style Emoticons (^.^ , .__. , ^^ , etc)  \n",
    "The nltk TweetTokenizer is well written and structured so I took it and customise it to add regex for Japanese style Emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'some', u'*__*', u'and', u'^.^', u'.__.', u'.___.', u'also', u'.___.', u'>_<', u'^^']\n"
     ]
    }
   ],
   "source": [
    "import py_compile\n",
    "py_compile.compile(\"custom_tokenize.py\")\n",
    "\n",
    "import Preprocessor\n",
    "from Preprocessor import tokenise\n",
    "\n",
    "print tokenise(\"some *__* and ^.^, .__., .___. also .________.   >_< ^^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'picture',\n",
       " u'translation',\n",
       " u'sina',\n",
       " u'interview',\n",
       " u'with',\n",
       " u'hyung',\n",
       " u'jun',\n",
       " u'^^',\n",
       " u'video',\n",
       " u'thanks',\n",
       " u'to',\n",
       " u'hyungjunthebest',\n",
       " u'AT_USER',\n",
       " u'LINK']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(\"[Picture, Translation] Sina Interview with Hyung Jun ^^video thanks to hyungjunthebest@youtube http://t.co/x5TLedQl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object at 0x10f1f7ed0>\n"
     ]
    }
   ],
   "source": [
    "EMOTICONS = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      <3                         # heart\n",
    "      |\n",
    "      [\\*#\\O~\\-=>][_\\.]+[\\*#\\O~\\-=<]      # Japanese style Emoticons eye mouth eye\n",
    "      |\n",
    "      [\\.][_]+[\\.]               # poker face\n",
    "      |\n",
    "      \\^[_.;]?\\^                 # smiling eyes\n",
    "    )\"\"\"\n",
    "\n",
    "print re.compile(EMOTICONS, re.VERBOSE | re.I | re.UNICODE ).match('^^')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object at 0x10f252030>\n",
      "<_sre.SRE_Match object at 0x10f252030>\n",
      "<_sre.SRE_Match object at 0x10f252030>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print re.match(r\"@.*\", '@someone')\n",
    "print re.match(r\"#.*\", '#sdf')\n",
    "print re.match(r\"http.*\", 'https://sdfdsf')\n",
    "print re.match(r\"http.*\", 'sdfhttp://')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip handles for @user and http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'AT_USER',\n",
       " u'this',\n",
       " u'is',\n",
       " u'AWESOME',\n",
       " u'!',\n",
       " u'!',\n",
       " u'NUMBER',\n",
       " u's',\n",
       " u'get',\n",
       " u'the',\n",
       " u'#party',\n",
       " u'started',\n",
       " u'!',\n",
       " u'LINK']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(\"@user this is AWESOME!!1Let's get the#party started!http://someurl.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise problem:  \n",
    "we can tokenise smilies like :),   \n",
    "but we cannot process things like ^.^ and \"\\-\\_\\_\\-\",  \n",
    "but we tokenise any emoticon as \\_\\_, which might not be a bad thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine the most common words in the training set B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common in positive [(u'!', 2582), (u'the', 2373), (u'NUMBER', 1796), (u'to', 1413), (u'AT_USER', 1387), (u'in', 905), (u'on', 874), (u'and', 829), (u'I', 764), (u'a', 764), (u'for', 733), (u'of', 693), (u'you', 629), (u'tomorrow', 589), (u'is', 583), (u'at', 558), (u'...', 523), (u'LINK', 515), (u'be', 507), (u'with', 484)]\n",
      "most common in neutral [(u'the', 3444), (u'NUMBER', 3038), (u'to', 1764), (u'AT_USER', 1517), (u'in', 1326), (u'on', 1291), (u'LINK', 1238), (u'!', 1219), (u'...', 1026), (u'and', 999), (u'of', 981), (u'for', 890), (u'a', 883), (u'at', 873), (u'with', 695), (u'?', 677), (u'I', 656), (u'is', 653), (u'tomorrow', 609), (u'you', 533)]\n",
      "most common in negative [(u'the', 918), (u'NUMBER', 602), (u'to', 524), (u'AT_USER', 388), (u'I', 387), (u'a', 360), (u'in', 325), (u'!', 321), (u'on', 313), (u'and', 309), (u'is', 298), (u'of', 277), (u'...', 262), (u'?', 246), (u'for', 216), (u'be', 193), (u'tomorrow', 182), (u'it', 178), (u'you', 174), (u'with', 161)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter \n",
    "\n",
    "tweets={'positive':[], 'negative':[], 'neutral':[]}\n",
    "dictionary = {'positive':Counter(), 'negative':Counter(), 'neutral':Counter()}\n",
    "word_dict=Counter()\n",
    "\n",
    "tfile = \"Exercise2_data/twitter-train-cleansed-B.tsv\"\n",
    "with open(tfile) as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        tclass = line[2]\n",
    "        tweet = line[3]\n",
    "        tweets[tclass].append(tweet)\n",
    "        tokens = tokenise(tweet)   \n",
    "        dictionary[tclass].update(tokens)\n",
    "        word_dict.update(tokens)\n",
    "\n",
    "for key, value in dictionary.iteritems():\n",
    "    print \"most common in\", key, value.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examing the windows in  some tweets in task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(700, 1, 1, 'positive', [u'congratulations']),\n",
       " (701, 13, 13, 'positive', [u'happy']),\n",
       " (702, 9, 9, 'positive', [u':D']),\n",
       " (703, 19, 19, 'positive', [u':)']),\n",
       " (704, 23, 23, 'negative', [u'DEATH']),\n",
       " (705, 5, 5, 'negative', [u'worst']),\n",
       " (706, 1, 4, 'neutral', [u'is', u'going', u'to', u'change']),\n",
       " (707, 20, 20, 'neutral', [u'as']),\n",
       " (708, 11, 11, 'negative', [u'for']),\n",
       " (709, 6, 7, 'positive', [u'fun-filled', u'day']),\n",
       " (710, 0, 2, 'positive', [u'so', u'proud', u'of']),\n",
       " (711, 7, 7, 'negative', [u'upset']),\n",
       " (712, 3, 3, 'positive', [u'great']),\n",
       " (713, 2, 2, 'positive', [u'lucky']),\n",
       " (714, 9, 12, 'positive', [u'DELIGHTED', u'what', u'a', u'SATURDAY']),\n",
       " (715, 14, 14, 'positive', []),\n",
       " (716, 2, 2, 'positive', [u'intelligent']),\n",
       " (717, 5, 5, 'positive', [u'mindfulness']),\n",
       " (718, 12, 12, 'positive', [u'glorious']),\n",
       " (719, 17, 19, 'positive', [u'congratulations', u'to', u'egypt'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import linecache\n",
    "import functools\n",
    "\n",
    "def inspectTweetsA(filepath, linenumbers, tokeniser):\n",
    "    tweets = []\n",
    "    for linenumber in linenumbers:\n",
    "        line = linecache.getline(filepath, linenumber+1).split('\\t')\n",
    "        begin = int(line[2])\n",
    "        end = int(line[3])\n",
    "        label = line[4]\n",
    "        tokens = tokeniser(line[5], begin=begin, end=end)\n",
    "        tweets.append((linenumber, begin, end, label, tokens))\n",
    "    return tweets    \n",
    "\n",
    "tokenise_func = functools.partial(tokenise , more_instances=0)\n",
    "twitter_train_A_path = \"Exercise2_data/twitter-train-cleansed-A.tsv\"\n",
    "\n",
    "tweets = inspectTweetsA(twitter_train_A_path, range(700,720), tokenise_func)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that tweet number 715 does not contain anything at word 14,  \n",
    "the tweet is \"I just remembered 'Niggas In Paris' is a song. DELIGHTED! What a SATURDAY!  #positivetroll\"  \n",
    "\n",
    "There are only 13 words in the tweet ignoring puncuation(as all the other lines) so it must have been an error in the training set.  \n",
    "\n",
    "I left it being empty because during training, if it does not contain any feature, it would not enter the for loop \"for fname in featureset:\" in all the classifier we are using, so it would not be counted, which is what we wanted.\n",
    "\n",
    "For the tokeniser, we add option to add before and after 1 token and it prove to improve performance of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K skip bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class extract_kskip_bigram(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, skip=1, hashing=False):\n",
    "        self.skip = skip\n",
    "        self.hashing = hashing\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tokenlist):\n",
    "        featurelist = []\n",
    "        \n",
    "        for tokens in tokenlist:\n",
    "            features = {}  \n",
    "            tokens = [ word.strip('#') for word in tokens ] #strip hashtag\n",
    "            \n",
    "            for s in range(self.skip+1):\n",
    "            \n",
    "                ngrams = zip(*[tokens[i:] for i in (0,1+s)])\n",
    "                for tup in ngrams:\n",
    "                    key = ' '.join(tup)\n",
    "                    if self.hashing:\n",
    "                        features[key] = True\n",
    "                    else:\n",
    "                        features[key] = features.get(key, 0) +1\n",
    "            featurelist.append(features)\n",
    "\n",
    "        return featurelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u\"we're\", u'going', u'to', u'be', u'in', u'the', u'rose', u'parade', u'tomorrow', u'with', u'the', u'other', u'obedience', u'club', u'dogs', u'!', u'lizzie', u'will', u'be', u'wearing', u'roses', u'!', u':-)'], [u'new', u'header', u'of', u'lana', u'del', u'rey', u'*__*', u'her', u'face', u'is', u'getting', u'blocked', u'though', u'I', u'may', u'change', u'it', u'later', u'..']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{u'! :-)': 1,\n",
       "  u'! lizzie': 1,\n",
       "  u'! will': 1,\n",
       "  u'be in': 1,\n",
       "  u'be roses': 1,\n",
       "  u'be the': 1,\n",
       "  u'be wearing': 1,\n",
       "  u'club !': 1,\n",
       "  u'club dogs': 1,\n",
       "  u'dogs !': 1,\n",
       "  u'dogs lizzie': 1,\n",
       "  u'going be': 1,\n",
       "  u'going to': 1,\n",
       "  u'in rose': 1,\n",
       "  u'in the': 1,\n",
       "  u'lizzie be': 1,\n",
       "  u'lizzie will': 1,\n",
       "  u'obedience club': 1,\n",
       "  u'obedience dogs': 1,\n",
       "  u'other club': 1,\n",
       "  u'other obedience': 1,\n",
       "  u'parade tomorrow': 1,\n",
       "  u'parade with': 1,\n",
       "  u'rose parade': 1,\n",
       "  u'rose tomorrow': 1,\n",
       "  u'roses !': 1,\n",
       "  u'roses :-)': 1,\n",
       "  u'the obedience': 1,\n",
       "  u'the other': 1,\n",
       "  u'the parade': 1,\n",
       "  u'the rose': 1,\n",
       "  u'to be': 1,\n",
       "  u'to in': 1,\n",
       "  u'tomorrow the': 1,\n",
       "  u'tomorrow with': 1,\n",
       "  u\"we're going\": 1,\n",
       "  u\"we're to\": 1,\n",
       "  u'wearing !': 1,\n",
       "  u'wearing roses': 1,\n",
       "  u'will be': 1,\n",
       "  u'will wearing': 1,\n",
       "  u'with other': 1,\n",
       "  u'with the': 1},\n",
       " {u'*__* face': 1,\n",
       "  u'*__* her': 1,\n",
       "  u'I change': 1,\n",
       "  u'I may': 1,\n",
       "  u'blocked I': 1,\n",
       "  u'blocked though': 1,\n",
       "  u'change it': 1,\n",
       "  u'change later': 1,\n",
       "  u'del *__*': 1,\n",
       "  u'del rey': 1,\n",
       "  u'face getting': 1,\n",
       "  u'face is': 1,\n",
       "  u'getting blocked': 1,\n",
       "  u'getting though': 1,\n",
       "  u'header lana': 1,\n",
       "  u'header of': 1,\n",
       "  u'her face': 1,\n",
       "  u'her is': 1,\n",
       "  u'is blocked': 1,\n",
       "  u'is getting': 1,\n",
       "  u'it ..': 1,\n",
       "  u'it later': 1,\n",
       "  u'lana del': 1,\n",
       "  u'lana rey': 1,\n",
       "  u'later ..': 1,\n",
       "  u'may change': 1,\n",
       "  u'may it': 1,\n",
       "  u'new header': 1,\n",
       "  u'new of': 1,\n",
       "  u'of del': 1,\n",
       "  u'of lana': 1,\n",
       "  u'rey *__*': 1,\n",
       "  u'rey her': 1,\n",
       "  u'though I': 1,\n",
       "  u'though may': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenlist = [tokenise(t3),tokenise(t4)]\n",
    "print tokenlist\n",
    "# for tokens in tokenlist:\n",
    "#     print zip(*[tokens[i:] for i in range(2)]).count((u\"we're\", u'going'))\n",
    "\n",
    "extract_kskip_bigram(skip=1).transform( tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"we're\", u'going', u'to', u'be', u'in', u'the', u'rose', u'parade', u'tomorrow', u'with', u'the', u'other', u'obedience', u'club', u'dogs', u'!', u'lizzie', u'will', u'be', u'wearing', u'roses', u'!', u':-)']\n",
      "[(u\"we're\", u'going'), (u'going', u'to'), (u'to', u'be'), (u'be', u'in'), (u'in', u'the'), (u'the', u'rose'), (u'rose', u'parade'), (u'parade', u'tomorrow'), (u'tomorrow', u'with'), (u'with', u'the'), (u'the', u'other'), (u'other', u'obedience'), (u'obedience', u'club'), (u'club', u'dogs'), (u'dogs', u'!'), (u'!', u'lizzie'), (u'lizzie', u'will'), (u'will', u'be'), (u'be', u'wearing'), (u'wearing', u'roses'), (u'roses', u'!'), (u'!', u':-)')]\n",
      "[(u\"we're\", u'to'), (u'going', u'be'), (u'to', u'in'), (u'be', u'the'), (u'in', u'rose'), (u'the', u'parade'), (u'rose', u'tomorrow'), (u'parade', u'with'), (u'tomorrow', u'the'), (u'with', u'other'), (u'the', u'obedience'), (u'other', u'club'), (u'obedience', u'dogs'), (u'club', u'!'), (u'dogs', u'lizzie'), (u'!', u'will'), (u'lizzie', u'be'), (u'will', u'wearing'), (u'be', u'roses'), (u'wearing', u'!'), (u'roses', u':-)')]\n",
      "[(u\"we're\", u'be'), (u'going', u'in'), (u'to', u'the'), (u'be', u'rose'), (u'in', u'parade'), (u'the', u'tomorrow'), (u'rose', u'with'), (u'parade', u'the'), (u'tomorrow', u'other'), (u'with', u'obedience'), (u'the', u'club'), (u'other', u'dogs'), (u'obedience', u'!'), (u'club', u'lizzie'), (u'dogs', u'will'), (u'!', u'be'), (u'lizzie', u'wearing'), (u'will', u'roses'), (u'be', u'!'), (u'wearing', u':-)')]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenlist[0]\n",
    "print tokens\n",
    "skip = 2\n",
    "for s in range(0, skip+1):\n",
    "    print zip(*[tokens[i:] for i in (0,1+s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sentiwordnet from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.25\n",
      "1.0\n",
      "0.0178571428571\n",
      "0.0357142857143\n",
      "0.946428571429\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "\n",
    "word = 'late'\n",
    "\n",
    "print max([x.pos_score() for x in list(swn.senti_synsets(word))])\n",
    "print max([x.neg_score() for x in list(swn.senti_synsets(word))])\n",
    "print max([x.obj_score() for x in list(swn.senti_synsets(word))])\n",
    "\n",
    "print np.mean([x.pos_score() for x in list(swn.senti_synsets(word))])\n",
    "print np.mean([x.neg_score() for x in list(swn.senti_synsets(word))])\n",
    "print np.mean([x.obj_score() for x in list(swn.senti_synsets(word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lexicon_sentiwordnet(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, feature='all'):\n",
    "        self.feature = feature\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    # helper method\n",
    "    def get_lexi_feature(self, features):\n",
    "        senti_list = lambda word: list(swn.senti_synsets(word)) \n",
    "        pos = sum([ max([0]+[ x.pos_score() for x in senti_list(word) ])*features[word] for word in features ])\n",
    "        neg = sum([ max([0]+[ x.neg_score() for x in senti_list(word) ])*features[word] for word in features ])\n",
    "        obj = sum([ max([0]+[ x.obj_score() for x in senti_list(word) ])*features[word] for word in features ])\n",
    "        if self.feature == 'all':\n",
    "            return {'swn_pos': pos, 'swn_neg': neg, 'swn_obj': obj }\n",
    "        else:\n",
    "            return {'swn_score': pos - neg }\n",
    "            \n",
    "    def transform(self, featurelist):\n",
    "        return [ self.get_lexi_feature(features) for features in featurelist ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'swn_score': 0.625}, {'swn_score': 0.75}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurelist = [{\n",
    "  u'be': 1,\n",
    "  u'lizzie': 1,\n",
    "  u'will': 1,\n",
    "  u'!': 1,\n",
    "  u'in': 1,\n",
    "  u'rose': 1,\n",
    "  u'roses': 1,\n",
    "  u'the': 1,\n",
    "  u'wearing': 1,\n",
    "  u'club': 1,\n",
    "  u'going': 1,\n",
    "  u'parade': 1},{\n",
    "  u'rose': 1,\n",
    "  u'obedience': 1,\n",
    "  u'other': 1,\n",
    "  u'tomorrow': 1,\n",
    "        u'happy': 1,\n",
    "}]\n",
    "\n",
    "lexicon_sentiwordnet(feature='score').transform(featurelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Liu Hu Lexicon from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from multiprocessing import Pool\n",
    "\n",
    "class lexicon_liuhu(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pos_lexicon = set(opinion_lexicon.positive())\n",
    "        self.neg_lexicon = set(opinion_lexicon.negative())\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, featurelist):\n",
    "        def get_pos_neg(features):\n",
    "            pos = sum(1 for word in features if word in self.pos_lexicon)\n",
    "            neg = sum(1 for word in features if word in self.neg_lexicon)\n",
    "            return [pos, neg]\n",
    "        \n",
    "        return [ get_pos_neg(features) for features in featurelist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 17.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "timeit lexicon_liuhu().transform(featurelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 and NRC Hashtag Lexicon Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Preprocessor\n",
    "import PipelineRunner\n",
    "\n",
    "reload(Preprocessor)\n",
    "gold_set = PipelineRunner.getTrainingSetB(\"Exercise2_data/twitter-dev-gold-B.tsv\", tokenise)\n",
    "lexi = Preprocessor.extract_kskip_bigram(skip=3).transform(gold_set['tokens'])\n",
    "lexi = Preprocessor.lexicon_NRC_bigram().transform(lexi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ (i, k) for i, k in enumerate(lexi) if sum(k)!=0 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'will',\n",
       " u'take',\n",
       " u'that',\n",
       " u'result',\n",
       " u'top',\n",
       " u'the',\n",
       " u'group',\n",
       " u'as',\n",
       " u'well',\n",
       " u'bet',\n",
       " u'tim',\n",
       " u'howard',\n",
       " u'is',\n",
       " u'cursing',\n",
       " u'away',\n",
       " u'at',\n",
       " u'us',\n",
       " u'ahead',\n",
       " u'of',\n",
       " u'sunday',\n",
       " u'#LFC']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_set['tokens'][1033]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean GloVe pre-train Twitter word vector\n",
    "the original word vector is 257.7MB for only 25 dimension  \n",
    "it contains a lots of non english words that we don't need in this task  \n",
    "here we extract only the enlish words and < user >, < link >, etc to speed up the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object at 0x120d0b100>\n",
      "<_sre.SRE_Match object at 0x120d0b100>\n",
      "<_sre.SRE_Match object at 0x120d0b100>\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ENG_RE = re.compile(r\"^[a-zA-Z]+$\")\n",
    "print ENG_RE.match(r\"i\")\n",
    "print ENG_RE.match(r\"you\")\n",
    "print ENG_RE.match(r\"somethejrwekewr\")\n",
    "print ENG_RE.match(r\"ayyaşlar\")\n",
    "print ENG_RE.match(r\"ﾖｲｼｮｯ\")\n",
    "print ENG_RE.match(r\"한강대교\")\n",
    "print ENG_RE.match(r\"nاحترم\")\n",
    "line = \"ciğerimiz -1.5423 -0.6015 1.4372 -0.81059 -1.4139 -0.238 -1.1359 -1.5304 -0.81756 0.023343 -1.0026 -0.95681 1.1536 1.2247 0.75876 -0.21646 0.98548 1.2462 0.61908 -0.45587 -0.054105 0.68484 -0.040623 0.04541 -1.0274\"\n",
    "print ENG_RE.match(line.split(\" \", 1)[0])\n",
    "line = \"nnこれをこうして -1.8114 0.31154 -1.804 2.3232 -1.4398 -0.34702 -0.69259 -0.90919 1.7543 -0.47194 -1.6467 -3.5453 2.3154 1.0312 -3.6022 -2.2001 -4.8098 -2.9928 0.99323 -1.2025 -0.37103 -4.3479 1.6415 -1.3526 0.3258\"\n",
    "print ENG_RE.match(line.split(\" \", 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for creating the cleaned file. Can only be runned if glove.twitter.27B.25d.txt is downloaded.  \n",
    "for twitter from glove.twitter.27B.zip http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Data downloaded form http://nlp.stanford.edu/data/glove.twitter.27B.zip  \n",
    "        (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)\n",
    "        The extracted file must be placed in the Lexicon_And_WE sub folder at the same folder of this file\n",
    "        And make sure the file path in the code exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# commented out because can only be run if glove.twitter.27B/glove.twitter.27B.25d.txt exist \n",
    "\n",
    "# import re\n",
    "\n",
    "# ENG_RE = re.compile(r\"^[a-zA-Z]+$\")\n",
    "# GLOVE_25d = 'Lexicon_And_WE/glove.twitter.27B/glove.twitter.27B.25d.txt'\n",
    "\n",
    "# target = open('Lexicon_And_WE/glove.twitter.27B/glove.twitter.25d-cleaned-en.txt', 'w')\n",
    "# with open(GLOVE_25d) as gfile:\n",
    "#     for line in gfile:\n",
    "#         word = line.split(' ', 1)[0] # split first occurance\n",
    "#         if ENG_RE.match(word):\n",
    "#             target.write(line)\n",
    "# target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test WE_GloVe_Twitter\n",
    "import Preprocessor\n",
    "import PipelineRunner\n",
    "\n",
    "reload(Preprocessor)\n",
    "gold_set = PipelineRunner.getTrainingSetB(\"Exercise2_data/twitter-dev-gold-B.tsv\", Preprocessor.tokenise)\n",
    "lexi = Preprocessor.WE_GloVe_Twitter().transform(gold_set['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.12192394117647061,\n",
       "  0.23052739999999999,\n",
       "  -0.30798617647058829,\n",
       "  -0.33501582352941173,\n",
       "  -0.068645552941176469,\n",
       "  0.0093505294117647107,\n",
       "  1.0058978823529412,\n",
       "  -0.096249941176470594,\n",
       "  -0.29775958823529408,\n",
       "  -0.24241999999999997,\n",
       "  -0.05634823529411765,\n",
       "  0.16948582352941177,\n",
       "  -4.0174423529411767,\n",
       "  0.41108117647058823,\n",
       "  0.39450341764705887,\n",
       "  0.080877647058823526,\n",
       "  0.034647176470588251,\n",
       "  -0.29263617647058821,\n",
       "  -0.4714964117647058,\n",
       "  -0.0014791176470588057,\n",
       "  -0.41170829999999997,\n",
       "  0.020581000000000002,\n",
       "  -0.024761117647058819,\n",
       "  -0.23383629411764706,\n",
       "  -0.089179529411764699,\n",
       "  -1.2971,\n",
       "  -0.70268,\n",
       "  -2.3364,\n",
       "  -1.5596,\n",
       "  -0.85091,\n",
       "  -1.2542,\n",
       "  -0.098218,\n",
       "  -1.2279,\n",
       "  -1.2761,\n",
       "  -1.3655,\n",
       "  -1.6672,\n",
       "  -2.501,\n",
       "  -6.3371,\n",
       "  -0.41398,\n",
       "  -0.5531,\n",
       "  -0.62011,\n",
       "  -0.71249,\n",
       "  -1.2579,\n",
       "  -2.5867,\n",
       "  -1.3695,\n",
       "  -1.4068,\n",
       "  -1.1655,\n",
       "  -0.94472,\n",
       "  -1.6099,\n",
       "  -0.90682,\n",
       "  1.5649,\n",
       "  0.85265,\n",
       "  0.7974,\n",
       "  0.87498,\n",
       "  1.4854,\n",
       "  1.3908,\n",
       "  2.2782,\n",
       "  2.3866,\n",
       "  0.42165,\n",
       "  0.74383,\n",
       "  0.77335,\n",
       "  1.4784,\n",
       "  -0.19662,\n",
       "  1.4328,\n",
       "  1.3562,\n",
       "  1.267,\n",
       "  1.2678,\n",
       "  0.88961,\n",
       "  0.63946,\n",
       "  1.2575,\n",
       "  0.46057,\n",
       "  1.184,\n",
       "  0.93483,\n",
       "  0.88524,\n",
       "  1.6189],\n",
       " [-0.04423601428571429,\n",
       "  0.21346742857142856,\n",
       "  0.070797642857142865,\n",
       "  -0.22207842857142859,\n",
       "  -0.35118207142857144,\n",
       "  -0.046691428571428571,\n",
       "  1.3630207142857143,\n",
       "  -0.48855071428571434,\n",
       "  -0.099942857142857139,\n",
       "  -0.28492771428571434,\n",
       "  0.16787214285714286,\n",
       "  0.3320285714285714,\n",
       "  -4.719035714285714,\n",
       "  0.43594214285714289,\n",
       "  0.29305180714285717,\n",
       "  -0.31151107142857143,\n",
       "  0.44845414285714286,\n",
       "  -0.37091565714285712,\n",
       "  -0.039554857142857128,\n",
       "  0.03721214285714286,\n",
       "  -0.6243344999999999,\n",
       "  0.046060000000000011,\n",
       "  0.11564192857142855,\n",
       "  -0.31086485714285716,\n",
       "  -0.071786214285714273,\n",
       "  -0.79154,\n",
       "  -0.61544,\n",
       "  -0.60656,\n",
       "  -1.6959,\n",
       "  -1.5527,\n",
       "  -0.83908,\n",
       "  0.24726,\n",
       "  -1.7553,\n",
       "  -0.52735,\n",
       "  -1.1523,\n",
       "  -0.67967,\n",
       "  -0.7644,\n",
       "  -6.3371,\n",
       "  -0.84409,\n",
       "  -0.27047,\n",
       "  -0.86441,\n",
       "  -0.58902,\n",
       "  -0.99077,\n",
       "  -1.1214,\n",
       "  -1.1685,\n",
       "  -1.7377,\n",
       "  -1.2414,\n",
       "  -0.6175,\n",
       "  -1.0477,\n",
       "  -0.58399,\n",
       "  0.39441,\n",
       "  0.85603,\n",
       "  0.62385,\n",
       "  0.69848,\n",
       "  0.47344,\n",
       "  0.73833,\n",
       "  1.8429,\n",
       "  0.56238,\n",
       "  0.62305,\n",
       "  0.65177,\n",
       "  0.95092,\n",
       "  0.95886,\n",
       "  -1.4781,\n",
       "  1.8983,\n",
       "  1.2071,\n",
       "  0.23183,\n",
       "  0.89976,\n",
       "  0.65325,\n",
       "  1.608,\n",
       "  1.1102,\n",
       "  0.2639,\n",
       "  0.83403,\n",
       "  1.1259,\n",
       "  0.5188,\n",
       "  1.1393]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(lexi)\n",
    "lexi[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
